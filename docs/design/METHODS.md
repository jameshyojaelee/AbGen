# Methods

## Preference Reward Function

We build preference pairs by scoring candidate sequences with a lightweight, transparent
reward that reflects antibody developability heuristics. The total reward is the negative
weighted sum of penalties (lower penalties → higher reward).

**Components:**

1. **Motif liability penalty**
   - Counts canonical liability motifs (e.g., N-glyc, deamidation, oxidation) via
     `abprop.utils.find_motifs` and normalizes by sequence length.
   - Penalty = sum of normalized motif counts.

2. **Non-canonical residue penalty**
   - Fraction of residues not in the canonical 20 amino acids.
   - Penalizes ambiguous or invalid residues (including `X`).

3. **Uncertainty penalty (optional)**
   - If a checkpoint is provided, we run MC dropout on the liability head and
     penalize the mean predictive variance across liabilities.

4. **Humanness proxy (optional)**
   - Simple length-range penalty: sequences outside a typical antibody range
     (default 90–130 residues) incur a normalized penalty.
   - This is a placeholder proxy until a more principled humanness model is integrated.

**Total reward:**

```
reward = - (w_motif * motif_penalty
            + w_noncanonical * noncanonical_penalty
            + w_uncertainty * uncertainty_penalty
            + w_humanness * humanness_penalty)
```

The reward builder lives in `src/abprop/rewards.py`, and preference datasets can be
constructed with `scripts/make_preferences.py`.

## Generation

AbGen currently provides an **MLM edit sampler** (`abprop.generation.sample_mlm_edit`).
Candidates are generated by repeatedly masking random positions and resampling tokens
with temperature/top-k/top-p controls. This aligns with the MLM training objective and
serves as a practical baseline for design workflows.

## DPO Alignment

DPO training is implemented in `abprop.train.dpo.DPOLoss` and exposed via
`scripts/train_dpo.py`. The policy and reference models score masked tokens, and the
DPO loss is optimized to increase preference-aligned likelihood.
