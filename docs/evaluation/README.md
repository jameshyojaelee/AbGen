# Evaluation

- Metrics and tables: `docs/evaluation/RESULTS.md`
- Leaderboard snapshot: `docs/evaluation/LEADERBOARD.md`
- Benchmarks: `scripts/run_benchmarks.py` (use `configs/benchmarks_local.yaml` for local data)
- Guardrails: `scripts/run_guardrails.py` expects benchmark JSONs under `outputs/guardrails/`
- Design benchmark output: `outputs/benchmarks/design/summary.json`
